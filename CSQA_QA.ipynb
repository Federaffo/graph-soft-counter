{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6EpLMhmmqhpTxuOr5Gojm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Federaffo/graph-soft-counter/blob/main/CSQA_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph Soft Counter Question answering"
      ],
      "metadata": {
        "id": "y7jbZb1fzW5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clono il repo"
      ],
      "metadata": {
        "id": "AMCiKlPizcnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZJKULLfyIvI",
        "outputId": "8a786dc8-ba47-44e0-b84c-c627fc72188e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'graph-soft-counter'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 47 (delta 15), reused 47 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kuan-wang/graph-soft-counter.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd graph-soft-counter/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WOqagDwJIxx",
        "outputId": "c582b0d7-c872-4956-870b-ffd418049a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/graph-soft-counter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download dei dati preprocessati\n",
        "\n",
        "Operazioni svolte durante il preprocessing\n",
        "\n",
        "*   Setup ConceptNet\n",
        "*   Conversione QA dataset in .json files\n",
        "*   Identificazione dei concetti \n",
        "*   Estrazione di un sottografo per ogni coppia q-a\n",
        "\n"
      ],
      "metadata": {
        "id": "ZF3wsQIZzyX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sh download_preprocessed_data.sh"
      ],
      "metadata": {
        "id": "ST7_4KbYy5Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch_geometric\n",
        "!pip install torch_sparse\n",
        "!pip install torch_scatter"
      ],
      "metadata": {
        "id": "Bc5zuagfLDan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training del modello"
      ],
      "metadata": {
        "id": "XKTRnWKr0YZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A causa del superamento del limite di memoria di Colab l'esecuzione è stata interrotta all'epoca 14"
      ],
      "metadata": {
        "id": "77--ViPO0jpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sh run_gsc__csqa.sh --train_adj --train_statements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeD1dRLQJExH",
        "outputId": "0ec285c5-f086-422e-a6e5-909076ce73ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** hyperparameters *****\n",
            "dataset: csqa\n",
            "enc_name: roberta-large\n",
            "batch_size: 128\n",
            "learning_rate: elr 1e-5 dlr 1e-2\n",
            "edge_encoder_dim 32 gsc_layer 2\n",
            "******************************\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "Moving 0 files to the new cache system\n",
            "0it [00:00, ?it/s]\n",
            "8bc693936edf\n",
            "pid: 743\n",
            "screen: \n",
            "\n",
            "gpu: 0\n",
            "\n",
            "Namespace(batch_size=128, counter_type='gsc', cuda=True, dataset='csqa', debug=False, decoder_lr=0.01, dev_adj='data/csqa/graph/dev.graph.adj.pk', dev_statements='data/csqa/statement/dev.statement.jsonl', dropoutf=0.0, enc_dim=32, encoder='roberta-large', encoder_layer=-1, encoder_lr=1e-05, ent_emb=['tzw'], ent_emb_paths=['data/cpnet/tzw.ent.npy'], eval_batch_size=8, fc_dim=512, fc_layer_num=0, inhouse=True, inhouse_train_qids='data/csqa/inhouse_split_qids.txt', init_range=0.02, k=2, load_model_path=None, load_sentvecs_model_path=None, log_interval=20, loss='cross_entropy', lr_schedule='fixed', max_epochs_before_stop=10, max_grad_norm=1.0, max_node_num=32, max_seq_len=88, mini_batch_size=4, mode='train', n_epochs=30, num_relation=38, optim='radam', refreeze_epoch=10000, save_dir='saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404', save_model=True, seed=0, simple=False, subsample=1.0, test_adj='data/csqa/graph/test.graph.adj.pk', test_statements='data/csqa/statement/test.statement.jsonl', train_adj='data/csqa/graph/train.graph.adj.pk', train_statements='data/csqa/statement/train.statement.jsonl', unfreeze_epoch=4, use_cache=True, warmup_steps=150, weight_decay=0.001, without_amp=False)\n",
            "train_statement_path data/csqa/statement/train.statement.jsonl\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 6.12MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 3.78MB/s]\n",
            "Downloading: 100% 482/482 [00:00<00:00, 476kB/s]\n",
            "num_choice 5\n",
            "loading adj matrices:   0% 0/48705 [00:00<?, ?it/s]/content/graph-soft-counter/utils/data_utils.py:142: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  i, j = ij // n_node, ij % n_node\n",
            "loading adj matrices: 100% 48705/48705 [00:51<00:00, 946.71it/s]\n",
            "| ori_adj_len: mu 121.54 sigma 94.04 | adj_len: 10.49 | prune_rate： 0.99 | qc_num: 7.42 | ac_num: 2.07 |\n",
            "loading adj matrices: 100% 6105/6105 [00:06<00:00, 957.99it/s]\n",
            "| ori_adj_len: mu 118.44 sigma 90.55 | adj_len: 10.26 | prune_rate： 1.00 | qc_num: 7.20 | ac_num: 2.05 |\n",
            "loading adj matrices: 100% 5700/5700 [00:05<00:00, 960.21it/s]\n",
            "| ori_adj_len: mu 119.20 sigma 93.49 | adj_len: 10.43 | prune_rate： 1.00 | qc_num: 7.38 | ac_num: 2.05 |\n",
            "max train seq length:  77\n",
            "max dev seq length:  71\n",
            "max test seq length:  88\n",
            "------------------------------\n",
            "using  roberta-large\n",
            "------------------------------\n",
            "Downloading: 100% 1.43G/1.43G [00:23<00:00, 59.9MB/s]\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "parameters:\n",
            "\tgnn.edge_encoder.0.layers.0-Linear.weight    \ttrainable\ttorch.Size([32, 46])\tdevice:cuda:0\n",
            "\tgnn.edge_encoder.0.layers.0-Linear.bias      \ttrainable\ttorch.Size([32])\tdevice:cuda:0\n",
            "\tgnn.edge_encoder.0.layers.0-LayerNorm.weight \ttrainable\ttorch.Size([32])\tdevice:cuda:0\n",
            "\tgnn.edge_encoder.0.layers.0-LayerNorm.bias   \ttrainable\ttorch.Size([32])\tdevice:cuda:0\n",
            "\tgnn.edge_encoder.0.layers.1-Linear.weight    \ttrainable\ttorch.Size([1, 32])\tdevice:cuda:0\n",
            "\tgnn.edge_encoder.0.layers.1-Linear.bias      \ttrainable\ttorch.Size([1])\tdevice:cuda:0\n",
            "\tgnn.regulator.layers.0-Linear.weight         \ttrainable\ttorch.Size([32, 1])\tdevice:cuda:0\n",
            "\tgnn.regulator.layers.0-Linear.bias           \ttrainable\ttorch.Size([32])\tdevice:cuda:0\n",
            "\tgnn.regulator.layers.0-LayerNorm.weight      \ttrainable\ttorch.Size([32])\tdevice:cuda:0\n",
            "\tgnn.regulator.layers.0-LayerNorm.bias        \ttrainable\ttorch.Size([32])\tdevice:cuda:0\n",
            "\tgnn.regulator.layers.1-Linear.weight         \ttrainable\ttorch.Size([1, 32])\tdevice:cuda:0\n",
            "\tgnn.regulator.layers.1-Linear.bias           \ttrainable\ttorch.Size([1])\tdevice:cuda:0\n",
            "\tfc.layers.0-Linear.weight                    \ttrainable\ttorch.Size([1, 1024])\tdevice:cuda:0\n",
            "\tfc.layers.0-Linear.bias                      \ttrainable\ttorch.Size([1])\tdevice:cuda:0\n",
            "\ttotal: 2787\n",
            "\n",
            "-----------------------------------------------------------------------\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "| step    19 |  lr: 0.0000100 | loss  1.6086 | ms/batch 3299.01 |\n",
            "| step    39 |  lr: 0.0000100 | loss  1.6071 | ms/batch 3381.57 |\n",
            "| step    59 |  lr: 0.0000100 | loss  1.5898 | ms/batch 3340.92 |\n",
            "100% 153/153 [01:49<00:00,  1.40it/s]\n",
            "100% 156/156 [01:51<00:00,  1.40it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   0 | step    67 | dev_acc  0.2965 | test_acc  0.2804 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   0 | step    67 | best_dev_acc  0.2965 | final_test_acc  0.2804 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step    79 |  lr: 0.0000100 | loss  1.5785 | ms/batch 2188.72 |\n",
            "| step    99 |  lr: 0.0000100 | loss  1.5545 | ms/batch 3360.38 |\n",
            "| step   119 |  lr: 0.0000100 | loss  1.5135 | ms/batch 3366.57 |\n",
            "100% 153/153 [01:49<00:00,  1.39it/s]\n",
            "100% 156/156 [01:51<00:00,  1.40it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   1 | step   134 | dev_acc  0.3284 | test_acc  0.3094 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   1 | step   134 | best_dev_acc  0.3284 | final_test_acc  0.3094 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   139 |  lr: 0.0000100 | loss  1.4986 | ms/batch 1003.48 |\n",
            "| step   159 |  lr: 0.0000100 | loss  1.4926 | ms/batch 3355.19 |\n",
            "| step   179 |  lr: 0.0000100 | loss  1.4986 | ms/batch 3348.88 |\n",
            "| step   199 |  lr: 0.0000100 | loss  1.4899 | ms/batch 3347.98 |\n",
            "100% 153/153 [01:49<00:00,  1.40it/s]\n",
            "100% 156/156 [01:51<00:00,  1.40it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   2 | step   201 | dev_acc  0.3382 | test_acc  0.3054 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   2 | step   201 | best_dev_acc  0.3382 | final_test_acc  0.3054 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   219 |  lr: 0.0000100 | loss  1.4874 | ms/batch 3189.20 |\n",
            "| step   239 |  lr: 0.0000100 | loss  1.4872 | ms/batch 3329.18 |\n",
            "| step   259 |  lr: 0.0000100 | loss  1.4933 | ms/batch 3341.14 |\n",
            "100% 153/153 [01:49<00:00,  1.40it/s]\n",
            "100% 156/156 [01:50<00:00,  1.41it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   3 | step   268 | dev_acc  0.3342 | test_acc  0.3014 |\n",
            "-----------------------------------------------------------------------\n",
            "| step   279 |  lr: 0.0000100 | loss  1.4863 | ms/batch 5229.17 |\n",
            "| step   299 |  lr: 0.0000100 | loss  1.4971 | ms/batch 8776.30 |\n",
            "| step   319 |  lr: 0.0000100 | loss  1.4873 | ms/batch 8794.77 |\n",
            "100% 153/153 [01:54<00:00,  1.34it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   4 | step   335 | dev_acc  0.3342 | test_acc  0.2990 |\n",
            "-----------------------------------------------------------------------\n",
            "| step   339 |  lr: 0.0000100 | loss  1.4736 | ms/batch 2197.74 |\n",
            "| step   359 |  lr: 0.0000100 | loss  1.4725 | ms/batch 8761.27 |\n",
            "| step   379 |  lr: 0.0000100 | loss  1.4894 | ms/batch 8762.12 |\n",
            "| step   399 |  lr: 0.0000100 | loss  1.5085 | ms/batch 8775.06 |\n",
            "100% 153/153 [01:54<00:00,  1.34it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   5 | step   402 | dev_acc  0.3358 | test_acc  0.3022 |\n",
            "-----------------------------------------------------------------------\n",
            "| step   419 |  lr: 0.0000100 | loss  1.5178 | ms/batch 7897.11 |\n",
            "| step   439 |  lr: 0.0000100 | loss  1.4911 | ms/batch 8768.73 |\n",
            "| step   459 |  lr: 0.0000100 | loss  1.4869 | ms/batch 8767.57 |\n",
            "100% 153/153 [01:54<00:00,  1.34it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   6 | step   469 | dev_acc  0.3448 | test_acc  0.3151 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   6 | step   469 | best_dev_acc  0.3448 | final_test_acc  0.3151 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   479 |  lr: 0.0000100 | loss  1.4856 | ms/batch 4824.54 |\n",
            "| step   499 |  lr: 0.0000100 | loss  1.4790 | ms/batch 8766.57 |\n",
            "| step   519 |  lr: 0.0000100 | loss  1.4916 | ms/batch 8774.59 |\n",
            "100% 153/153 [01:55<00:00,  1.32it/s]\n",
            "100% 156/156 [01:57<00:00,  1.32it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   7 | step   536 | dev_acc  0.3473 | test_acc  0.3255 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   7 | step   536 | best_dev_acc  0.3473 | final_test_acc  0.3255 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   539 |  lr: 0.0000100 | loss  1.4898 | ms/batch 1756.92 |\n",
            "| step   559 |  lr: 0.0000100 | loss  1.4894 | ms/batch 8773.16 |\n",
            "| step   579 |  lr: 0.0000100 | loss  1.4823 | ms/batch 8770.19 |\n",
            "| step   599 |  lr: 0.0000100 | loss  1.4726 | ms/batch 8767.24 |\n",
            "100% 153/153 [01:54<00:00,  1.33it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   8 | step   603 | dev_acc  0.3694 | test_acc  0.3417 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   8 | step   603 | best_dev_acc  0.3694 | final_test_acc  0.3417 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   619 |  lr: 0.0000100 | loss  1.4423 | ms/batch 7458.35 |\n",
            "| step   639 |  lr: 0.0000100 | loss  1.4293 | ms/batch 8774.41 |\n",
            "| step   659 |  lr: 0.0000100 | loss  1.4391 | ms/batch 8763.70 |\n",
            "100% 153/153 [01:54<00:00,  1.33it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   9 | step   670 | dev_acc  0.4988 | test_acc  0.4512 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch   9 | step   670 | best_dev_acc  0.4988 | final_test_acc  0.4512 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   679 |  lr: 0.0000100 | loss  1.3539 | ms/batch 4393.66 |\n",
            "| step   699 |  lr: 0.0000100 | loss  1.2638 | ms/batch 8779.67 |\n",
            "| step   719 |  lr: 0.0000100 | loss  1.1875 | ms/batch 8765.18 |\n",
            "100% 153/153 [01:54<00:00,  1.33it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  10 | step   737 | dev_acc  0.6822 | test_acc  0.6438 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  10 | step   737 | best_dev_acc  0.6822 | final_test_acc  0.6438 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   739 |  lr: 0.0000100 | loss  1.1067 | ms/batch 1317.87 |\n",
            "| step   759 |  lr: 0.0000100 | loss  1.0088 | ms/batch 8788.29 |\n",
            "| step   779 |  lr: 0.0000100 | loss  0.9708 | ms/batch 8767.89 |\n",
            "| step   799 |  lr: 0.0000100 | loss  0.9450 | ms/batch 8760.63 |\n",
            "100% 153/153 [01:54<00:00,  1.34it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  11 | step   804 | dev_acc  0.7404 | test_acc  0.6994 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  11 | step   804 | best_dev_acc  0.7404 | final_test_acc  0.6994 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   819 |  lr: 0.0000100 | loss  0.8586 | ms/batch 7027.19 |\n",
            "| step   839 |  lr: 0.0000100 | loss  0.8113 | ms/batch 8780.88 |\n",
            "| step   859 |  lr: 0.0000100 | loss  0.8195 | ms/batch 8772.98 |\n",
            "100% 153/153 [01:54<00:00,  1.34it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  12 | step   871 | dev_acc  0.7527 | test_acc  0.7164 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  12 | step   871 | best_dev_acc  0.7527 | final_test_acc  0.7164 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   879 |  lr: 0.0000100 | loss  0.8114 | ms/batch 3953.01 |\n",
            "| step   899 |  lr: 0.0000100 | loss  0.7043 | ms/batch 8768.18 |\n",
            "| step   919 |  lr: 0.0000100 | loss  0.7137 | ms/batch 8761.21 |\n",
            "100% 153/153 [01:54<00:00,  1.34it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  13 | step   938 | dev_acc  0.7707 | test_acc  0.7293 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  13 | step   938 | best_dev_acc  0.7707 | final_test_acc  0.7293 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step   939 |  lr: 0.0000100 | loss  0.7057 | ms/batch  875.18 |\n",
            "| step   959 |  lr: 0.0000100 | loss  0.6121 | ms/batch 8779.13 |\n",
            "| step   979 |  lr: 0.0000100 | loss  0.6267 | ms/batch 8755.06 |\n",
            "| step   999 |  lr: 0.0000100 | loss  0.5996 | ms/batch 8760.08 |\n",
            "100% 153/153 [01:54<00:00,  1.34it/s]\n",
            "100% 156/156 [01:56<00:00,  1.34it/s]\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  14 | step  1005 | dev_acc  0.7748 | test_acc  0.7486 |\n",
            "-----------------------------------------------------------------------\n",
            "| epoch  14 | step  1005 | best_dev_acc  0.7748 | final_test_acc  0.7486 |\n",
            "model saved to saved_models/csqa/enc-roberta-large__k2_encdim32_bs128__seed0_20220919_121404/model.pt\n",
            "| step  1019 |  lr: 0.0000100 | loss  0.5602 | ms/batch 6577.77 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HVsSoOVNKt5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}